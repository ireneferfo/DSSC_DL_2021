{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python39264bitpytorchconda807e1ce0df8b49aaa2d280b862ff3f2b",
   "display_name": "Python 3.9.2 64-bit ('pytorch': conda)"
  },
  "metadata": {
   "interpreter": {
    "hash": "473297526cef0b63cde6061c30fc5781ec0366ecb219ec17eb118d28b3bc0202"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Homework 5\n",
    "\n",
    "Starting from the implementation contained within the notebook `05-pruning.ipynb`, extend the  `magnitude_pruning` function to allow for incremental (iterative) pruning. In the current case, if you try pruning one more time, you'll notice that it will not work as there's no way to communicate to the future calls of `magnitude_pruning` to ignore the parameters which have already been pruned. Find a way to enhance the routine s.t. it can effectively prune networks in a sequential fashion (i.e., if we passed an MLP already pruned of 20% of its parameters, we want to prune *another* 20% of parameters)."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "First, import all libraries and modules needed."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from scripts import mnist, train_utils, architectures, train\n",
    "from scripts.train_utils import accuracy, AverageMeter\n",
    "from scripts.torch_utils import use_gpu_if_possible"
   ]
  },
  {
   "source": [
    "My improved `magnitude_pruning` function: "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "def magnitude_pruning(model, pruning_rate, layers_to_prune, mask=None): \n",
    "    # mask is the starting mask for the process\n",
    "\n",
    "    if mask is None:\n",
    "\n",
    "        params_to_prune = [pars[1] for pars in model.named_parameters() if any([l in pars[0] for l in layers_to_prune])]\n",
    "        flat = torch.cat([pars.abs().flatten() for pars in params_to_prune], dim=0)\n",
    "\n",
    "        flat = flat.sort()[0]\n",
    "\n",
    "        position = int(pruning_rate * flat.shape[0])\n",
    "        thresh = flat[position]\n",
    "\n",
    "        mask = []\n",
    "        for pars in model.named_parameters():\n",
    "            if any([l in pars[0] for l in layers_to_prune]):\n",
    "                m = torch.where(pars[1].abs() >= thresh, 1, 0)\n",
    "                mask.append(m)\n",
    "                pars[1].data *= m\n",
    "            else:\n",
    "                mask.append(torch.ones_like(pars[1]))\n",
    "\n",
    "        return mask\n",
    "\n",
    "    else:\n",
    "        params_to_prune = [m*params for (name, params),m in zip(model.named_parameters(), mask) \n",
    "                            if any([layer in name for layer in layers_to_prune])]\n",
    "        flat = torch.cat([pars.abs().flatten() for pars in params_to_prune], dim=0)\n",
    "        flat = flat.sort()[0]\n",
    "        flat = flat[flat.nonzero()]\n",
    "        position = int(pruning_rate* flat.shape[0])\n",
    "        thresh = flat[position]\n",
    "\n",
    "        new_mask = []\n",
    "        for i, ((name, param),m) in enumerate(zip(model.named_parameters(), mask)):\n",
    "            if any([layer in name for layer in layers_to_prune]):\n",
    "                new_m = torch.where(m*param.abs() >= thresh, 1, 0)\n",
    "                new_mask.append(new_m)\n",
    "                param.data *= new_m\n",
    "            else:\n",
    "                new_mask.append(torch.ones_like(param))\n",
    "\n",
    "        return new_mask "
   ]
  },
  {
   "source": [
    "Let's see if it works. From the provided notebook `05-pruning.ipynb`:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, dataloader, loss_fn, optimizer, loss_meter, performance_meter, performance, device, mask, layers_to_prune, params_type_to_prune):\n",
    "    for X, y in dataloader:\n",
    "        X = X.to(device)\n",
    "        y = y.to(device)\n",
    "\n",
    "        optimizer.zero_grad() \n",
    "        y_hat = model(X)\n",
    "        loss = loss_fn(y_hat, y)\n",
    "        loss.backward()\n",
    "\n",
    "        if mask is not None:\n",
    "            for (name, param), m in zip(model.named_parameters(), mask):\n",
    "                if any([l in name for l in layers_to_prune]):\n",
    "                    param.grad *= m\n",
    "\n",
    "        optimizer.step()\n",
    "        acc = performance(y_hat, y)\n",
    "\n",
    "        loss_meter.update(val=loss.item(), n=X.shape[0])\n",
    "        performance_meter.update(val=acc, n=X.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataloader, loss_fn, optimizer, num_epochs, checkpoint_loc=None, checkpoint_name=\"checkpoint.pt\", performance=accuracy, lr_scheduler=None, device=None, mask=None, layers_to_prune=None, params_type_to_prune=[\"weight\", \"bias\"]):\n",
    "    if checkpoint_loc is not None:\n",
    "        os.makedirs(checkpoint_loc, exist_ok=True)\n",
    "\n",
    "    if device is None:\n",
    "        device = use_gpu_if_possible()\n",
    "    \n",
    "    model = model.to(device)\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        loss_meter = AverageMeter()\n",
    "        performance_meter = AverageMeter()\n",
    "\n",
    "        print(f\"Epoch {epoch+1} --- learning rate {optimizer.param_groups[0]['lr']:.5f}\")\n",
    "\n",
    "        train_epoch(model, dataloader, loss_fn, optimizer, loss_meter, performance_meter, performance, device, mask, layers_to_prune, params_type_to_prune)\n",
    "\n",
    "        print(f\"Epoch {epoch+1} completed. Loss - total: {loss_meter.sum} - average: {loss_meter.avg}; Performance: {performance_meter.avg}\")\n",
    "\n",
    "        if checkpoint_name is not None and checkpoint_loc is not None:\n",
    "            checkpoint_dict = {\n",
    "                \"parameters\": model.state_dict(),\n",
    "                \"optimizer\": optimizer.state_dict(),\n",
    "                \"epoch\": epoch\n",
    "            }\n",
    "            torch.save(checkpoint_dict, os.path.join(checkpoint_loc, checkpoint_name))\n",
    "        \n",
    "        if lr_scheduler is not None:\n",
    "            lr_scheduler.step()\n",
    "\n",
    "    return loss_meter.sum, performance_meter.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "MLPCustom(\n  (layers): Sequential(\n    (0): Flatten(start_dim=1, end_dim=-1)\n    (1): Linear(in_features=784, out_features=16, bias=True)\n    (2): ReLU()\n    (3): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (4): Linear(in_features=16, out_features=32, bias=True)\n    (5): ReLU()\n    (6): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (7): Linear(in_features=32, out_features=64, bias=True)\n    (8): ReLU()\n    (9): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (10): Linear(in_features=64, out_features=10, bias=True)\n    (11): ReLU()\n  )\n)\n"
     ]
    }
   ],
   "source": [
    "layers = [\n",
    "    {\"n_in\": 784, \"n_out\": 16, \"batchnorm\": False},\n",
    "    {\"n_out\": 32, \"batchnorm\": True},\n",
    "    {\"n_out\": 64, \"batchnorm\": True},\n",
    "    {\"n_out\": 10, \"batchnorm\": True}\n",
    "]\n",
    "net = architectures.MLPCustom(layers)\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "def number_of_ones_in_mask(mask):\n",
    "    return sum([m.sum().item() for m in mask]) / sum([m.numel() for m in mask])"
   ]
  },
  {
   "source": [
    "Iterative pruning:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader, testloader, _, _ = mnist.get_data()\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1 --- learning rate 0.10000\n",
      "Epoch 1 completed. Loss - total: 24872.93850517273 - average: 0.41454897508621213; Performance: 0.8872833333333333\n",
      "Epoch 2 --- learning rate 0.10000\n",
      "Epoch 2 completed. Loss - total: 12579.992371559143 - average: 0.2096665395259857; Performance: 0.939\n",
      "Epoch 3 --- learning rate 0.10000\n",
      "Epoch 3 completed. Loss - total: 10631.141635417938 - average: 0.17718569392363231; Performance: 0.9486\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(10631.141635417938, 0.9486)"
      ]
     },
     "metadata": {},
     "execution_count": 153
    }
   ],
   "source": [
    "train_model(net, trainloader, loss_fn, optimizer, num_epochs=3, layers_to_prune=[\"1\", \"4\", \"7\", \"10\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Number of ones in mask: 0.8027967681789931 \n\n"
     ]
    }
   ],
   "source": [
    "mask = magnitude_pruning(net, 0.2, set([\"1\", \"4\", \"7\", \"10\"]))\n",
    "print(\"Number of ones in mask:\", number_of_ones_in_mask(mask), \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1 --- learning rate 0.10000\n",
      "Epoch 1 completed. Loss - total: 9261.125999450684 - average: 0.15435209999084473; Performance: 0.9545833333333333\n",
      "Epoch 2 --- learning rate 0.10000\n",
      "Epoch 2 completed. Loss - total: 8554.511769771576 - average: 0.1425751961628596; Performance: 0.9583166666666667\n",
      "Epoch 3 --- learning rate 0.10000\n",
      "Epoch 3 completed. Loss - total: 8190.981409549713 - average: 0.13651635682582855; Performance: 0.9587833333333333\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(8190.981409549713, 0.9587833333333333)"
      ]
     },
     "metadata": {},
     "execution_count": 155
    }
   ],
   "source": [
    "train_model(net, trainloader, loss_fn, optimizer, num_epochs=3, layers_to_prune=[\"1\", \"4\", \"7\", \"10\"], mask=mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Number of ones in mask: 0.6450590428837788 \n\n"
     ]
    }
   ],
   "source": [
    "mask = magnitude_pruning(net, 0.2, set([\"1\", \"4\", \"7\", \"10\"]), mask=mask)\n",
    "print(\"Number of ones in mask:\", number_of_ones_in_mask(mask), \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1 --- learning rate 0.10000\n",
      "Epoch 1 completed. Loss - total: 7544.220559358597 - average: 0.12573700932264328; Performance: 0.9625166666666667\n",
      "Epoch 2 --- learning rate 0.10000\n",
      "Epoch 2 completed. Loss - total: 7218.545625925064 - average: 0.12030909376541774; Performance: 0.9628\n",
      "Epoch 3 --- learning rate 0.10000\n",
      "Epoch 3 completed. Loss - total: 7017.527235031128 - average: 0.1169587872505188; Performance: 0.965\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(7017.527235031128, 0.965)"
      ]
     },
     "metadata": {},
     "execution_count": 157
    }
   ],
   "source": [
    "train_model(net, trainloader, loss_fn, optimizer, num_epochs=3, layers_to_prune=[\"1\", \"4\", \"7\", \"10\"], mask=mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Number of ones in mask: 0.5188315724052206 \n\n"
     ]
    }
   ],
   "source": [
    "mask = magnitude_pruning(net, 0.2, set([\"1\", \"4\", \"7\", \"10\"]),  mask=mask)\n",
    "print(\"Number of ones in mask:\", number_of_ones_in_mask(mask), \"\\n\")"
   ]
  },
  {
   "source": [
    "_Conclusion_: The number of ones is the mask is reduced by 20% at every iteration, so it works."
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}