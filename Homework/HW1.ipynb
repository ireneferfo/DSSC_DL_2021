{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DL -HW1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CseKHKAsQ5bD"
      },
      "source": [
        "# Homework 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9fWNWWCcRC6D"
      },
      "source": [
        "### 1. Taking inspiration from the notebook `01-intro-to-pt.ipynb`, build a class for the Multilayer Perceptron (MLP) whose scheme is drawn in the last figure of the notebook. As written there, no layer should have bias units and the activation for each hidden layer should be the Rectified Linear Unit (ReLU) function, also called ramp function. The activation leading to the output layer, instead, should be the softmax function, which prof. Ansuini explained during the last lecture. You can find some notions on it also on the notebook.                    \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2e63-5SBUeNn"
      },
      "source": [
        "import torch\n",
        "import numpy as np"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XSHjiZH8RyPy"
      },
      "source": [
        "class MLP(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.layer1 = torch.nn.Linear(in_features=5, out_features=11, bias = False)\n",
        "        self.layer2 = torch.nn.Linear(in_features=11, out_features=16, bias = False)\n",
        "        self.layer3 = torch.nn.Linear(in_features=16, out_features=13, bias = False)\n",
        "        self.layer4 = torch.nn.Linear(in_features=13, out_features=8, bias = False)\n",
        "        self.layer5 = torch.nn.Linear(in_features=8, out_features=4, bias = False)\n",
        "\n",
        "    def forward(self, X):\n",
        "        out = self.layer1(X)\n",
        "        out = torch.nn.functional.relu(out)\n",
        "        out = self.layer2(out)\n",
        "        out = torch.nn.functional.relu(out)\n",
        "        out = self.layer3(out)\n",
        "        out = torch.nn.functional.relu(out)\n",
        "        out = self.layer4(out)\n",
        "        out = torch.nn.functional.relu(out)\n",
        "        out = self.layer5(out)\n",
        "        out = torch.nn.functional.softmax(out)\n",
        "        return out"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LW9SnWnLRGQF"
      },
      "source": [
        "\n",
        "### 2. After having defined the class, create an instance of it and print a summary using a method of your choice.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XXPEHyNkRyy3",
        "outputId": "19189598-4e2b-479d-d94d-00730fa185ef"
      },
      "source": [
        "import sys\n",
        "!{sys.executable} -m pip install torch-summary #how to use pip or conda in jupyter notebooks\n",
        "from torchsummary import summary"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch-summary in /usr/local/lib/python3.7/dist-packages (1.4.5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "16IC7-QRVJPm",
        "outputId": "043795b7-c2a9-467a-8fa4-5620adfdf26e"
      },
      "source": [
        "from random import seed \n",
        "seed(123456)\n",
        "model = MLP()\n",
        "summary(model)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "=================================================================\n",
            "Layer (type:depth-idx)                   Param #\n",
            "=================================================================\n",
            "├─Linear: 1-1                            55\n",
            "├─Linear: 1-2                            176\n",
            "├─Linear: 1-3                            208\n",
            "├─Linear: 1-4                            104\n",
            "├─Linear: 1-5                            32\n",
            "=================================================================\n",
            "Total params: 575\n",
            "Trainable params: 575\n",
            "Non-trainable params: 0\n",
            "=================================================================\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "=================================================================\n",
              "Layer (type:depth-idx)                   Param #\n",
              "=================================================================\n",
              "├─Linear: 1-1                            55\n",
              "├─Linear: 1-2                            176\n",
              "├─Linear: 1-3                            208\n",
              "├─Linear: 1-4                            104\n",
              "├─Linear: 1-5                            32\n",
              "=================================================================\n",
              "Total params: 575\n",
              "Trainable params: 575\n",
              "Non-trainable params: 0\n",
              "================================================================="
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GcIjHreNRGSL"
      },
      "source": [
        "\n",
        "### 3. Provide detailed calculations (layer-by-layer) on the exact number of parameters in the network.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aXxHLoiPXhOC"
      },
      "source": [
        "A set of parentheses for each layer, without bias units:\n",
        "$$(5 \\times 11)+(11 \\times 16) + (16 \\times 13) + (13 \\times 8) + (8 \\times 4) = 55 + 176 + 208 + 104 + 32 = 575$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z3u2e36rRjap"
      },
      "source": [
        "### Provide the same calculation in the case that the bias units are present in all layers (except input)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "warMPeFPRzXY"
      },
      "source": [
        "A set of parentheses for each layer, in each the additive term represents the biases.\n",
        "\n",
        "$$(5 \\times 11+11)+(11 \\times 16+16) + (16 \\times 13 +13) + (13 \\times 8+8) + (8 \\times 4 +4) = 66 + 192 + 221 + 112 + 36 = 627$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ERMwc5gbRl30"
      },
      "source": [
        "### 4. For each layer within the MLP, calculate the L2 norm and L1 norm of its parameters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hwB5FkUlR0iK",
        "outputId": "5c90a924-a7c0-4cad-d7f6-850faba6af4d"
      },
      "source": [
        "# L2: Sum of the squared weights\n",
        "# L1: Sum of the absolute weights\n",
        "\n",
        "i = 0\n",
        "for param in model.parameters():\n",
        "  print(\"LAYER\", i)\n",
        "  print(\"Norm L1:\", torch.norm(param, 1).item())\n",
        "  print(\"Norm L2:\", torch.norm(param, 2).item())\n",
        "  print(\"\")\n",
        "  i+=1"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "LAYER 0\n",
            "Norm L1: 13.130066871643066\n",
            "Norm L2: 2.0066583156585693\n",
            "\n",
            "LAYER 1\n",
            "Norm L1: 27.10474395751953\n",
            "Norm L2: 2.398174524307251\n",
            "\n",
            "LAYER 2\n",
            "Norm L1: 26.631126403808594\n",
            "Norm L2: 2.150670289993286\n",
            "\n",
            "LAYER 3\n",
            "Norm L1: 15.996356964111328\n",
            "Norm L2: 1.7539299726486206\n",
            "\n",
            "LAYER 4\n",
            "Norm L1: 5.739292621612549\n",
            "Norm L2: 1.175723671913147\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}